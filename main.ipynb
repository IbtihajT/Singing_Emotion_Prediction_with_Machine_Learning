{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Jupyter Audio Playback\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Others\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pyloudnorm as pyln\n",
    "import pandas as pd\n",
    "import opensmile\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay, hamming_loss, jaccard_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# import matplotlib as mpl\n",
    "# mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(path):\n",
    "    \"\"\"\n",
    "    Parse datapoint information from file name and extract features from audio file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get file name\n",
    "    file_name = path.split('/')[-1]\n",
    "\n",
    "    # Split the file name into its respective identifiers\n",
    "    identifiers = file_name.split('.')[0].split('-')\n",
    "\n",
    "    # Feature configuration by RAVDESS https://zenodo.org/record/1188976#.ZBmfly-l1lz\n",
    "    feature_modes = {\n",
    "        \"modality\": {\n",
    "            \"01\": \"full_AV\",\n",
    "            \"02\": \"video_only\",\n",
    "            \"03\": \"audio_only\"\n",
    "        },\n",
    "        \"vocal_channel\": {\n",
    "            \"01\": \"speech\",\n",
    "            \"02\": \"song\"\n",
    "        },\n",
    "        \"emotion\": {\n",
    "            \"01\": \"neutral\",\n",
    "            \"02\": \"calm\",\n",
    "            \"03\": \"happy\",\n",
    "            \"04\": \"sad\",\n",
    "            \"05\": \"angry\",\n",
    "            \"06\": \"fearful\",\n",
    "            \"07\": \"disgust\",\n",
    "            \"08\": \"surprised\",\n",
    "        },\n",
    "        \"emotional_intensity\": {\n",
    "            \"01\": \"normal\",\n",
    "            \"02\": \"strong\"\n",
    "        },\n",
    "        \"statement\": {\n",
    "            \"01\": \"Kids are talking by the door\",\n",
    "            \"02\": \"Dogs are sitting by the door\",\n",
    "        },\n",
    "        \"repetition\": {\n",
    "            \"01\": \"1st repetition\",\n",
    "            \"02\": \"2nd repetition\"\n",
    "        },\n",
    "        \"actor\": {\n",
    "            \"odd\": \"male\",\n",
    "            \"even\": \"female\"\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "    # Generate Row\n",
    "    row = dict()\n",
    "\n",
    "    # Add file name to row\n",
    "    row[\"file_name\"] = f\"{file_name}\"\n",
    "\n",
    "    # Add audio vactor and features to row\n",
    "    y, sr = librosa.load(path, sr=48000)\n",
    "    row[\"audio_vector\"] = y\n",
    "    row[\"sample_rate\"] = sr\n",
    "\n",
    "    # Add features to row\n",
    "    for identifier, feature_name in zip(identifiers[:-1], list(feature_modes.keys())[:-1]):\n",
    "        row[feature_name] = feature_modes[feature_name][identifier]\n",
    "    \n",
    "    # Add gender to row\n",
    "    if int(identifiers[-1]) % 2 == 0:\n",
    "        row[\"gender\"] = \"female\"\n",
    "    else:\n",
    "        row[\"gender\"] = \"male\"\n",
    "\n",
    "    return pd.Series(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_audio_arrays(audio_arrays, desired_length, top_db):\n",
    "    \"\"\"\n",
    "    Clips the audio arrays to the desired length and pads them equally on both sides.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Clipping and Padding Process Started\")\n",
    "\n",
    "    # Initialize a list to store the clipped arrays\n",
    "    clipped_arrays = []\n",
    "\n",
    "    for audio_array in tqdm(audio_arrays):\n",
    "        # Detect the non-silent segments\n",
    "        non_silent_segments = librosa.effects.split(audio_array, top_db=top_db)\n",
    "\n",
    "        # Find the start and end indices of the non-silent segments\n",
    "        start_index = non_silent_segments[0, 0]\n",
    "        end_index = non_silent_segments[-1, 1]\n",
    "\n",
    "        # Clip the array\n",
    "        clipped_array = audio_array[start_index:end_index]\n",
    "\n",
    "        # Clip the array from both sides if it exceeds the desired length\n",
    "        if len(clipped_array) > desired_length:\n",
    "            excess = len(clipped_array) - desired_length\n",
    "            clipped_array = clipped_array[excess//2 : -excess//2]\n",
    "\n",
    "        # Calculate the amount of padding on each side\n",
    "        padding = desired_length - len(clipped_array)\n",
    "        padding_left = max(padding // 2, 0)\n",
    "        padding_right = max(padding - padding_left, 0)\n",
    "\n",
    "        # Pad the array equally on both ends\n",
    "        padded_array = np.pad(\n",
    "            clipped_array, (padding_left, padding_right), mode='constant'\n",
    "        )\n",
    "\n",
    "        # Add the padded array to the list\n",
    "        clipped_arrays.append(padded_array)\n",
    "    \n",
    "    print(\"Clipping and Padding Process Completed\")\n",
    "\n",
    "    return clipped_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Maximum Loudness\n",
    "def compute_maximum_loudness(audio_vectors):\n",
    "    \"\"\"\n",
    "    Computes the maximum loudness of the audio vectors.\n",
    "    \"\"\"\n",
    "    print(\"Computing Maximum Loudness\")\n",
    "    \n",
    "    meter = pyln.Meter(48000)\n",
    "    loudness = [meter.integrated_loudness(audio_vector) for audio_vector in audio_vectors]\n",
    "\n",
    "    print(\"Maximum Loudness Computed\")\n",
    "    return np.array(loudness).max()\n",
    "\n",
    "# Peak Normalize with LUFS, Max Loudness Provided\n",
    "def peak_normalize(audio_vector, max_loudness=None):\n",
    "    if max_loudness is None:\n",
    "        meter = pyln.Meter(48000)\n",
    "        max_loudness = meter.integrated_loudness(audio_vector)\n",
    "    return pyln.normalize.loudness(audio_vector, max_loudness, -23.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset paths\n",
    "paths = sorted(glob(\"./data/Audio_Song_Actors_01-24/*/*\"))\n",
    "\n",
    "# Extract features\n",
    "file_information = [extract_features(path) for path in paths]\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame(file_information)\n",
    "df.set_index(\"file_name\", inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Plots of all features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "for ax, column in zip(axes.flatten(), df.columns[3:]):\n",
    "    sns.countplot(data=df, x=column, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot Each Audio Vector\n",
    "# print(\"Plotting Audio Vectors\")\n",
    "# for audio_vector in tqdm(df[\"audio_vector\"]):\n",
    "#     plt.plot(audio_vector)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak Normalisation with LUFS\n",
    "# Get the Maximum Loudness\n",
    "maximum_loudness = compute_maximum_loudness(df[\"audio_vector\"].values)\n",
    "\n",
    "# Normalise the audio vectors\n",
    "print(\"Peak Normalising Audio Vectors\")\n",
    "normalised_audio_vectors = df[\"audio_vector\"].apply(peak_normalize, args=(maximum_loudness,))\n",
    "print(\"Peak Normalisation Completed\")\n",
    "\n",
    "# Update the original Dataframe\n",
    "df[\"audio_vector\"] = normalised_audio_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clipping and Padding\n",
    "clipped_padded_vectors = pd.Series(\n",
    "    clip_audio_arrays(\n",
    "        df[\"audio_vector\"].values, \n",
    "        desired_length=int(df[\"audio_vector\"].map(len).mean()), \n",
    "        top_db=30\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Replace the audio vectors with the clipped and padded vectors in original dataframe\n",
    "df[\"audio_vector\"] = clipped_padded_vectors.values\n",
    "\n",
    "# # Audio overlap plot to check if the audio is clipped and padded properly\n",
    "# print(\"Plotting Audio Overlap\")\n",
    "# for audio in tqdm(df[\"audio_vector\"]):\n",
    "#     pd.Series(audio).plot(\n",
    "#     figsize=(16, 5), \n",
    "#     lw=0.5,\n",
    "#     ylabel=\"Amplitude\",\n",
    "#     xlabel=\"Time\",\n",
    "#     title=\"Audio Overlap\",\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure openSMILE\n",
    "smile = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "    feature_level=opensmile.FeatureLevel.LowLevelDescriptors,\n",
    "    multiprocessing=True,\n",
    "    num_workers=32,\n",
    "    verbose=True,\n",
    "    sampling_rate=48000,\n",
    ")\n",
    "\n",
    "# Extract features\n",
    "features_list = []\n",
    "for audio_vector in tqdm(df[\"audio_vector\"]):\n",
    "\n",
    "        # Compute OpenSMILE features\n",
    "        features = smile.process_signal(audio_vector, sampling_rate=48000)\n",
    "\n",
    "        # Extract min, mean, std, max and rename the columns\n",
    "        min = features.min().add_suffix(\"_min\")\n",
    "        means = features.mean().add_suffix(\"_mean\")\n",
    "        stds = features.std().add_suffix(\"_std\")\n",
    "        max = features.max().add_suffix(\"_max\")\n",
    "\n",
    "        # Concatenate the features\n",
    "        features = pd.concat([min, means, stds, max], axis=0)\n",
    "\n",
    "        # Append the features to the list\n",
    "        features_list.append(features)\n",
    "\n",
    "# Features and Targets Dataframes\n",
    "features_df = pd.DataFrame(features_list)\n",
    "targets_df = df[\"emotion\"].astype(\"category\")\n",
    "combined_df = features_df.assign(target=targets_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features (Z-Score Normalization)\n",
    "feature_standard = StandardScaler()\n",
    "feature_standard.fit(features_df)\n",
    "normal_features_df = pd.DataFrame(feature_standard.transform(features_df), columns=features_df.columns)\n",
    "\n",
    "# One Hot Encode the Targets\n",
    "label_encoded_targets_df = pd.DataFrame(targets_df.cat.codes, columns=[\"target\"])\n",
    "oneHotEncoder = OneHotEncoder().fit(label_encoded_targets_df)\n",
    "one_hot_encoded_targets_df = pd.DataFrame(oneHotEncoder.transform(label_encoded_targets_df).toarray(), columns=targets_df.cat.categories)\n",
    "\n",
    "# Train Test Split - Label Encoded Targets\n",
    "X_train, X_test, y_train, y_test = train_test_split(normal_features_df, targets_df, test_size=0.1, random_state=42)\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training Labels shape: {y_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Testing Labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Logistic Regression\n",
    "\n",
    "classifier = LogisticRegression(class_weight='balanced', solver=\"saga\", max_iter=5000, n_jobs=-1, random_state=42)\n",
    "print()\n",
    "print(\"----------Training Logistic Regression----------\")\n",
    "print()\n",
    "classifier.fit(X_train, y_train.squeeze())\n",
    "\n",
    "# Pridiciton of Testset\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "classif_report = classification_report(y_test, y_pred, target_names=targets_df.cat.categories.values)\n",
    "cm = confusion_matrix(y_test, y_pred, labels=classifier.classes_)\n",
    "\n",
    "print('{:<23}: {:>10.2f}'.format('Accuracy Score', acc_score), sep='')\n",
    "print(classif_report)\n",
    "print()\n",
    "\n",
    "cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=targets_df.cat.categories.values)\n",
    "cm_disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# Get Logistic Regression Top Features\n",
    "lr_top_features = set(X_train.loc[:, SelectFromModel(classifier, prefit=True, max_features=150).get_support()].columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) SVC\n",
    "\n",
    "classifier = SVC(kernel='linear', max_iter=-1, class_weight='balanced', random_state=42)\n",
    "print()\n",
    "print(\"----------Training Support Vector Classification----------\")\n",
    "print()\n",
    "classifier.fit(X_train, y_train.squeeze())\n",
    "\n",
    "# Pridiciton of Testset\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "classif_report = classification_report(y_test, y_pred, target_names=targets_df.cat.categories.values,)\n",
    "cm = confusion_matrix(y_test, y_pred, labels=classifier.classes_)\n",
    "\n",
    "print('{:<23}: {:>10.2f}'.format('Accuracy Score', acc_score), sep='')\n",
    "print(classif_report)\n",
    "print()\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=targets_df.cat.categories.values)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# Get Support Vector Classifier Top Features\n",
    "svc_top_features = set(X_train.loc[:, SelectFromModel(classifier, prefit=True, max_features=150).get_support()].columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) NuSVC\n",
    "\n",
    "classifier = NuSVC(max_iter=-1, class_weight='balanced', kernel='rbf', random_state=42, nu=0.08)\n",
    "print()\n",
    "print(\"----------Training Nu-Support Vector Classification----------\")\n",
    "print()\n",
    "classifier.fit(X_train, y_train.squeeze())\n",
    "\n",
    "# Pridiciton of Testset\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "classif_report = classification_report(y_test, y_pred, target_names=targets_df.cat.categories.values,)\n",
    "cm = confusion_matrix(y_test, y_pred, labels=classifier.classes_)\n",
    "\n",
    "print('{:<23}: {:>10.2f}'.format('Accuracy Score', acc_score), sep='')\n",
    "print(classif_report)\n",
    "print()\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=targets_df.cat.categories.values)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Random Forest\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=500, n_jobs=-1, class_weight='balanced', random_state=42)\n",
    "print()\n",
    "print(\"----------Training Random Forest----------\")\n",
    "print()\n",
    "classifier.fit(X_train, y_train.squeeze())\n",
    "\n",
    "# Pridiciton of Testset\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "classif_report = classification_report(y_test, y_pred, target_names=targets_df.cat.categories.values)\n",
    "cm = confusion_matrix(y_test, y_pred, labels=classifier.classes_)\n",
    "\n",
    "print('{:<23}: {:>10.2f}'.format('Accuracy Score', acc_score), sep='')\n",
    "print(classif_report)\n",
    "print()\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=targets_df.cat.categories.values)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# Get Random Forest Classifier Top Features\n",
    "rf_top_features = set(X_train.loc[:, SelectFromModel(classifier, prefit=True, max_features=150).get_support()].columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Extra Trees\n",
    "\n",
    "classifier = ExtraTreesClassifier(n_estimators=1000, n_jobs=-1, class_weight='balanced', random_state=42)\n",
    "print()\n",
    "print(\"----------Training Extra Trees Classifier----------\")\n",
    "print()\n",
    "classifier.fit(X_train, y_train.squeeze())\n",
    "\n",
    "# Pridiciton of Testset\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "classif_report = classification_report(y_test, y_pred, target_names=targets_df.cat.categories.values)\n",
    "cm = confusion_matrix(y_test, y_pred, labels=classifier.classes_)\n",
    "\n",
    "print('{:<23}: {:>10.2f}'.format('Accuracy Score', acc_score), sep='')\n",
    "print(classif_report)\n",
    "print()\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=targets_df.cat.categories.values)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# Get Random Forest Classifier Top Features\n",
    "et_top_features = set(X_train.loc[:, SelectFromModel(classifier, prefit=True, max_features=150).get_support()].columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split - One Hot Encoded Targets\n",
    "X_train, X_test, y_train, y_test = train_test_split(normal_features_df, one_hot_encoded_targets_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Update the features dataframe with the top features\n",
    "# Intersect the top features of all the classifiers\n",
    "top_features = set.intersection(lr_top_features, svc_top_features, rf_top_features, et_top_features)\n",
    "X_train = X_train.loc[:, list(top_features)]\n",
    "X_test = X_test.loc[:, list(top_features)]\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training Labels shape: {y_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Testing Labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(nInputFeatures):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(tf.keras.layers.Dense(8, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    model.build(input_shape=(None, nInputFeatures),)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Model\n",
    "model = get_model(nInputFeatures=len(top_features))\n",
    "\n",
    "log_dir = \"tensorboard_logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "# Fit the Model\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=64,\n",
    "    epochs=1000,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test, y_test),\n",
    "    use_multiprocessing=True,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n",
    "\n",
    "# Plot the loss and accuracy curves for training and validation\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r',\n",
    "           label=\"Validation loss\", axes=ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_accuracy'],\n",
    "           color='r', label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved Model\n",
    "model = tf.keras.models.load_model(\"./deep89.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves for training and validation\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r',\n",
    "           label=\"Validation loss\", axes=ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_accuracy'],\n",
    "           color='r', label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pridiciton of Testset\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = oneHotEncoder.inverse_transform(y_pred).squeeze()\n",
    "y_test_inverse = oneHotEncoder.inverse_transform(y_test).squeeze()\n",
    "\n",
    "# Confusion Matrix\n",
    "acc_score = accuracy_score(y_test_inverse, y_pred)\n",
    "classif_report = classification_report(y_test_inverse, y_pred, target_names=targets_df.cat.categories.values,)\n",
    "cm = confusion_matrix(y_test_inverse, y_pred)\n",
    "\n",
    "print('{:<23}: {:>10.2f}'.format('Accuracy Score', acc_score), sep='')\n",
    "print(classif_report)\n",
    "print()\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=targets_df.cat.categories.values)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
